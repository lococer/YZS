{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import jieba\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "data = pd.read_csv('merged_partial.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank(sentence, lower=True, window=2, topK=20, withWeight=False, allowPOS=()):\n",
    "    \"\"\"\n",
    "    TextRank关键词提取\n",
    "    :param sentence: str，句子，文本内容\n",
    "    :param lower: bool，是否转小写处理\n",
    "    :param window: int，共现窗口大小\n",
    "    :param topK: int，返回几个TF值最高的词\n",
    "    :param withWeight: bool，是否返回每个关键词的权重\n",
    "    :param allowPOS: tuple，仅包括指定词性的词，默认为空，即不进行筛选\n",
    "    :return: list，关键词列表\n",
    "    \"\"\"\n",
    "    if lower:\n",
    "        sentence = sentence.lower()\n",
    "    \n",
    "    wordNet = jieba.lcut(sentence)\n",
    "    G = nx.Graph()\n",
    "    wordNet = [word for word in wordNet if word not in jieba.analyse.extract_tags(sentence, topK=100)]\n",
    "    \n",
    "    G.add_nodes_from(wordNet)\n",
    "    \n",
    "    for idx, word in enumerate(wordNet):\n",
    "        previous_word = wordNet[max(0, idx-window)]\n",
    "        for i in range(1, window+1):\n",
    "            following_word = wordNet[idx+i] if idx+i < len(wordNet) else \"\"\n",
    "            if following_word and (not allowPOS or any([wordNet[idx].startswith(pos) for pos in allowPOS])):\n",
    "                G.add_edge(previous_word, word)\n",
    "            if following_word and (not allowPOS or any([following_word.startswith(pos) for pos in allowPOS])):\n",
    "                G.add_edge(word, following_word)\n",
    "    \n",
    "    if G.number_of_edges() == 0: return []\n",
    "    scores = nx.pagerank(G)\n",
    "    \n",
    "    if withWeight:\n",
    "        return sorted(list(scores.items()), key=lambda x: x[1], reverse=True)[:topK]\n",
    "    else:\n",
    "        return sorted(scores, key=scores.get, reverse=True)[:topK]\n",
    "\n",
    "# 示例使用\n",
    "text = \"自然语言处理是人工智能和语言学领域的分支学科。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。\"\n",
    "keywords = textrank(text, topK=5)\n",
    "print(keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征提取\n",
    "def extract_keywords(summary, top_k=6):\n",
    "    if pd.isnull(summary):  # 检查是否为空值\n",
    "        return []\n",
    "    words = jieba.lcut(summary)\n",
    "    tf_idf_vectorizer = TfidfVectorizer()\n",
    "    tf_idf_vectorizer.fit([summary])\n",
    "    scores = tf_idf_vectorizer.idf_\n",
    "    features = tf_idf_vectorizer.get_feature_names_out()\n",
    "    sorted_scores = sorted(zip(features, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [word for word, score in sorted_scores[:top_k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将剧情简介转换为关键词\n",
    "def get_keywords_from_summary(summary):\n",
    "    if pd.isnull(summary):\n",
    "        return \"\"\n",
    "    keywords = extract_keywords(summary, top_k=5)\n",
    "    return ' '.join(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keywords'] = data['summary'].apply(get_keywords_from_summary)\n",
    "\n",
    "# 向量化关键词\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "keyword_vectors = tfidf_vectorizer.fit_transform(data['keywords']).toarray()\n",
    "\n",
    "# 将向量转换为DataFrame\n",
    "keyword_df = pd.DataFrame(keyword_vectors, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# 将关键词向量DataFrame与原始数据合并\n",
    "data = pd.concat([data, keyword_df], axis=1)\n",
    "\n",
    "# 查看向量化结果\n",
    "data.to_csv('result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将其他文本特征（导演、编剧、演员、类型）向量化\n",
    "data['director_vector'] = tfidf_vectorizer.fit_transform(data['director_name'].astype(str).apply(lambda x: ' '.join(x))).toarray()\n",
    "data['author_vector'] = tfidf_vectorizer.fit_transform(data['author_name'].astype(str).apply(lambda x: ' '.join(x))).toarray()\n",
    "data['actor_vector'] = tfidf_vectorizer.fit_transform(data['actor_name'].astype(str).apply(lambda x: ' '.join(x))).toarray()\n",
    "data['genre_vector'] = tfidf_vectorizer.fit_transform(data['genre'].astype(str).apply(lambda x: ' '.join(x.split('/')))).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算特征权重相似度\n",
    "def calculate_feature_weight_similarity(feature_vectors, weights):\n",
    "    weighted_similarity = np.dot(feature_vectors, weights)\n",
    "    return weighted_similarity\n",
    "\n",
    "# 特征权重\n",
    "weights = np.array([0.5, 0.5, 0.5, 1, 1.5])  # 根据PDF文件中的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算电影之间的相似度\n",
    "data['movie_similarity'] = data.apply(lambda row: calculate_feature_weight_similarity(\n",
    "    np.hstack([row['director_vector'], row['author_vector'], row['actor_vector'], row['genre_vector'], row['keyword_vector']]),\n",
    "    weights), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算相似电影评分属性\n",
    "def calculate_similar_movie_scores(data, top_k=10):\n",
    "    similar_movie_scores = []\n",
    "    for index, row in data.iterrows():\n",
    "        similarities = row['movie_similarity']\n",
    "        top_similar_indices = np.argsort(similarities)[::-1][1:top_k+1]  # 排除自身\n",
    "        top_similar_scores = data.loc[top_similar_indices, 'rating']\n",
    "        similar_movie_score = top_similar_scores.mean()\n",
    "        similar_movie_scores.append(similar_movie_score)\n",
    "    return pd.Series(similar_movie_scores)\n",
    "\n",
    "data['similar_movie_score'] = calculate_similar_movie_scores(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 准备数据\n",
    "X = data.drop(['rating', 'movie_id', 'movie_similarity'], axis=1)\n",
    "y = data['rating']\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# 构建随机森林模型\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# 计算均方误差\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
